{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d17ea8f4-f1a5-4551-a147-bdc3d08c2e80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_station = spark.read.table(\"station\")\n",
    "\n",
    "dim_station = (df_station\n",
    "               .select(\"station_id\",\"name\",\"latitude\",\"longitude\")\n",
    "               )\n",
    "\n",
    "# To persist the result as a Delta table in Databricks\n",
    "dim_station.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dim_station\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d376d21c-f646-4509-9a6b-47d9f3537863",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import md5, concat_ws, year, month, dayofmonth, dayofweek\n",
    "\n",
    "df_payment = spark.read.table(\"payment\")\n",
    "dim_date = df_payment.select(\n",
    "    md5(concat_ws(\"\", df_payment[\"date\"].cast(\"string\"))).alias(\"date_id\"),\n",
    "    df_payment[\"date\"],\n",
    "    year(\"date\").alias(\"year\"),\n",
    "    ((month(\"date\") + 2) / 3).cast(\"int\").alias(\"quarter\"),\n",
    "    month(\"date\").alias(\"month\"),\n",
    "    dayofmonth(\"date\").alias(\"day\"),\n",
    "    dayofweek(\"date\").alias(\"weekday\")\n",
    ")\n",
    "\n",
    "dim_date.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dim_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07015407-3747-4fc5-adec-0eaa28c05f8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, year, current_date, sum, col\n",
    "\n",
    "df_rider = spark.read.table(\"rider\")\n",
    "df_payment = spark.read.table(\"payment\")\n",
    "df_trip = spark.read.table(\"trip\")\n",
    "\n",
    "# Join the DataFrames\n",
    "joined_df = (df_rider\n",
    "            .join(df_payment, \"rider_id\")\n",
    "            .join(df_trip, \"rider_id\"))\n",
    "\n",
    "# Perform the transformations\n",
    "dim_rider = (joined_df\n",
    "            .groupBy(\n",
    "                df_rider.rider_id,\n",
    "                df_rider[\"address\"],\n",
    "                df_rider[\"first\"],\n",
    "                df_rider[\"last\"],\n",
    "                df_rider.birthday,\n",
    "                df_rider.account_start_date,\n",
    "                df_rider.account_end_date,\n",
    "                df_rider.is_member)\n",
    "            .agg(\n",
    "                sum(df_payment.amount).alias(\"total_payed\"),\n",
    "                sum(datediff(df_trip.started_at, df_trip.ended_at)).alias(\"total_duration\"),\n",
    "                (datediff(current_date(), df_rider.birthday) / 365).alias(\"age_start_account\")\n",
    "            )\n",
    ")\n",
    "dim_rider.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dim_rider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5e2fb1a-dca6-4e1d-b549-87109bb0a6ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, minute, hour, md5, concat_ws\n",
    "\n",
    "df_trip = spark.read.table(\"trip\")\n",
    "\n",
    "fact_trip = df_trip.select(\n",
    "    col(\"trip_id\"),\n",
    "    col(\"rider_id\"),\n",
    "    col(\"start_station_id\"),\n",
    "    col(\"end_station_id\"),\n",
    "    (datediff(col(\"ended_at\"), col(\"started_at\")) * 24 * 60 + \n",
    "     (hour(col(\"ended_at\")) - hour(col(\"started_at\"))) * 60 + \n",
    "     (minute(col(\"ended_at\")) - minute(col(\"started_at\")))).alias(\"duration_minutes\"),\n",
    "    md5(concat_ws(\"\", col(\"started_at\").cast(\"string\"))).alias(\"date_start_id\"),\n",
    "    md5(concat_ws(\"\", col(\"ended_at\").cast(\"string\"))).alias(\"date_end_id\")\n",
    ")\n",
    "fact_trip.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"fact_trip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce926ee2-ed50-417a-a75e-70bdba39cb28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import md5, concat_ws\n",
    "\n",
    "df_payment = spark.read.table(\"payment\")\n",
    "\n",
    "fact_payment = df_payment.select(\n",
    "    col(\"payment_id\"),\n",
    "    col(\"rider_id\"),\n",
    "    md5(concat_ws(\"\", col(\"date\").cast(\"string\"))).alias(\"date_id\"),\n",
    "    col(\"amount\")\n",
    ")\n",
    "fact_payment.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"fact_payment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "350a391c-ece5-4686-8128-bdcd78e38505",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+----+-------+-----+---+-------+\n|             date_id|               date|year|quarter|month|day|weekday|\n+--------------------+-------------------+----+-------+-----+---+-------+\n|a772a0d1a8bd3662c...|2019-05-01 00:00:00|2019|      2|    5|  1|      4|\n|553969c5cf601b52d...|2019-06-01 00:00:00|2019|      2|    6|  1|      7|\n|c4149503a2ab5a01f...|2019-07-01 00:00:00|2019|      3|    7|  1|      2|\n|8d5503db9e4042bcf...|2019-08-01 00:00:00|2019|      3|    8|  1|      5|\n|c027f06afee66571b...|2019-09-01 00:00:00|2019|      3|    9|  1|      1|\n+--------------------+-------------------+----+-------+-----+---+-------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM dim_date LIMIT 5\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "TRANSFORM",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
